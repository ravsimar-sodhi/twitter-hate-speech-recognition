{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, GRU, Dropout, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import losses\n",
    "\n",
    "DATA_PATH=\"../data/\"\n",
    "DATA = DATA_PATH + \"labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter=',')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    data = [x[6] for x in raw]\n",
    "    return np.array(data)\n",
    "\n",
    "def getClass(raw):\n",
    "    classes = [x[5] for x in raw]\n",
    "    return np.array(classes)\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "    return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"f1 score: \", f1)\n",
    "    print(\"avg recall\", rec)    \n",
    "    print(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw = readData(DATA) \n",
    "tweets = getTweets(raw)\n",
    "classes = getClass(raw)\n",
    "tweets = preprocess(tweets)\n",
    "# print(len(tweets))\n",
    "# print(len(classes))\n",
    "print(tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"i'm probably your main bitch chocolate dipped cinnamon apple\" 1\n",
      "\"i'm sick of his shit man : sit his bitch ass, i'm fed up bro rt \": geno threw yet another pick smh\"\" 1\n",
      "\"i'm sorry i fucked yo bitch... but she really dig a nigga\"  1\n",
      "\"ibs\" irritable bitch syndrome. 1\n",
      "\"if you love her then hide yo bitch so you'll keep shorty.\"\n",
      " 1\n",
      "\"if you talk back to me you ain't getting no pussy.\"\n",
      "me: <url> 1\n",
      "\"if you're a bad bitch put your hands up high\" \n",
      "every girl in the room starts waving there hands around. 1\n",
      "\"imma tie that bitch up in the basement and donkey-fuck her ass for two weeks straight\" hahahahahahahahaha 50centmovie beforeiselfdestruct 1\n",
      "\"in my mind\"\n",
      "gotta be the dumb hoe anthem... 1\n",
      "\"is that an albino mexican?\"\n",
      "\"no dad, he's asian.\" 2\n",
      "\"it takes a true directioner to accept larry\" you know what bitch its fans like you that make the boys cry. thankyoupaulforconfirminglarry 1\n",
      "\"it's allegations, were u in the bedroom? did u see it?\" words of a naive &amp; blind bitch. we know ur husband cheated on u. ramona is dumb!!! 1\n",
      "\"it's never too late to tag a bitch.\"-  1\n",
      "\"it's the dykes\" -hunter dyke with a dyke <url> 1\n",
      "\"julia roberts, you're a ho\" nottinghill 1\n",
      "\"just follow the yellow brick twitter.\" 2\n",
      "\"lmfaoooooooooooooooooo : if your pussy is good but my mama don&8217;t like you? guess who gotta go&8230;&8230;&8230;..my mama.\" 1\n",
      "\"lol, cripple!\" - wendy davis. rt : scientists create remote-controlled chick to study shy penguins <url> 2\n",
      "\"let's do nips ahoy and spank me maybe\" 1\n",
      "\"let's kill cracker babies!\". wtf did i just hear???????? wow. 0\n",
      "\"like... correct me if i'm wrong but... doesn't pimpin involve 'smackin bitches for money'?\" 1\n",
      "\"lmao : &128514;&128514; rt : pound cake look like pussy &128514; <url>\" 1\n",
      "\"lmao : the d.rose adidas are lighter than my phone .... shit is retarded\" 1\n",
      "\"mam, ren&233;e doet in haar rol net of ze ongesteld is wat is ongesteld?\" van dat soort vragen waar je denkt: hoe leg ik die weer uit. 2\n",
      "\"maybe she wants to be more than your friend or maybe she's a stupid bitch. you never know until you make her cum.\" &128514;&128514;&128514; 1\n"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[1:]\n",
    "classes = classes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128 # In the original paper for character level convolutions, Zhang et al. used\n",
    "# a maxlen of 1014. Just using 1024, because for the sake of consitency, of comparison\n",
    "# with the next model. Also, the number 1014 kinda bothered me. 1024 makes me feel a lot better.\n",
    "nb_filter = 128\n",
    "dense_outputs = 256\n",
    "filter_kernels = [7, 7, 3, 3, 3, 3]\n",
    "n_out = 3\n",
    "# batch_size = 80\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 68\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "for doc in tweets:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, char_indices):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [char_indices[w] for w in sentences]\n",
    "        x2 = np.eye(len(char_indices))[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = vectorize_sentences(tweets, char_indices)\n",
    "X_train = data[:len(data) ]\n",
    "y = to_categorical(classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.2)\n",
    "# train_data.shape\n",
    "\n",
    "# classes.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9914, 128, 68)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_train = []\n",
    "# Y_test = []\n",
    "# for i in trainClass:\n",
    "#     if i == \"0\":\n",
    "#         Y_train.append([1, 0, 0])\n",
    "#     elif i == \"1\":\n",
    "#         Y_train.append([0, 1, 0])\n",
    "#     elif i == \"2\":\n",
    "#         Y_train.append([0, 0, 1])\n",
    "# Y_train = np.array(Y_train)\n",
    "\n",
    "# for i in testClass:\n",
    "#     if i == \"0\":\n",
    "#         Y_test.append([1, 0, 0])\n",
    "#     elif i == \"1\":\n",
    "#         Y_test.append([0, 1, 0])\n",
    "#     elif i == \"2\":\n",
    "#         Y_test.append([0, 0, 1])\n",
    "# Y_test = np.array(Y_test)\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", padding=\"valid\", input_shape=(256, 68), kernel_size=7, filters=128)`\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "  import sys\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=7, filters=128, padding=\"valid\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "  if sys.path[0] == '':\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128, padding=\"valid\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128, padding=\"valid\")`\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128, padding=\"valid\")`\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_size=3, filters=128, padding=\"valid\")`\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ou...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 250, 128)\n",
      "(?, 83, 128)\n",
      "(?, 3)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
    "\n",
    "conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0],\n",
    "                     border_mode='valid', activation='relu',\n",
    "                     input_shape=(maxlen, vocab_size))(inputs)\n",
    "print(conv.shape)\n",
    "conv = MaxPooling1D(pool_length=3)(conv)\n",
    "print(conv.shape)\n",
    "\n",
    "conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1],\n",
    "                      border_mode='valid', activation='relu')(conv)\n",
    "conv1 = MaxPooling1D(pool_length=3)(conv1)\n",
    "\n",
    "conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],\n",
    "                      border_mode='valid', activation='relu')(conv1)\n",
    "\n",
    "conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3],\n",
    "                      border_mode='valid', activation='relu')(conv2)\n",
    "\n",
    "conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4],\n",
    "                      border_mode='valid', activation='relu')(conv3)\n",
    "\n",
    "conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5],\n",
    "                      border_mode='valid', activation='relu')(conv4)\n",
    "conv5 = MaxPooling1D(pool_length=3)(conv5)\n",
    "conv5 = Flatten()(conv5)\n",
    "\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))\n",
    "z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
    "\n",
    "pred = Dense(n_out, batch_size=32, activation='softmax', name='output')(z) \n",
    "print(pred.shape)\n",
    "model = Model(input=inputs, output=pred)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 256, 68)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 250, 128)          61056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 83, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 77, 128)           114816    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 25, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 23, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 19, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 17, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               164096    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 603,651\n",
      "Trainable params: 603,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 256, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected output to have 2 dimensions, but got array with shape (4957, 256, 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ad8262482b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train, y_train, batch_size=32,\n\u001b[0;32m----> 3\u001b[0;31m            nb_epoch=10, validation_split=0.2, verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected output to have 2 dimensions, but got array with shape (4957, 256, 68)"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "model.fit(X_train, y_train, batch_size=32,\n",
    "           nb_epoch=10, validation_split=0.2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 17408 into shape (1,256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b6b076d03917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 17408 into shape (1,256)"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "# X_test = train_data[:4000]\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6851256974895188\n",
      "avg recall 0.3333333333333333\n",
      "accuracy 0.7811176114585435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'char',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(2, 6),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6851256974895188\n",
      "avg recall 0.3333333333333333\n",
      "accuracy 0.7811176114585435\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6851256974895188\n",
      "avg recall 0.3333333333333333\n",
      "accuracy 0.7811176114585435\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6851256974895188\n",
      "avg recall 0.3333333333333333\n",
      "accuracy 0.7811176114585435\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text To Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "allTweets = [x for x in tweets]\n",
    "classes = [x for x in classes]\n",
    "\n",
    "n = int(len(allTweets)*0.8)\n",
    "\n",
    "trainTweets = allTweets[1:n]\n",
    "testTweets = allTweets[n:]\n",
    "trainClass = classes[1:n]\n",
    "testClass = classes[n:]\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(trainTweets)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(trainTweets + testTweets)\n",
    "X_train = X[:n-1]\n",
    "X_test = X[n-1:]\n",
    "X_train = pad_sequences(X_train, maxlen = 32)\n",
    "X_test = pad_sequences(X_test, maxlen = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "for i in trainClass:\n",
    "    if i == \"0\":\n",
    "        Y_train.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_train.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_train.append([0, 0, 1])\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "for i in testClass:\n",
    "    if i == \"0\":\n",
    "        Y_test.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_test.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_test.append([0, 0, 1])\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\", use_bias=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", dropout=0.2, recurrent_dropout=0.2)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 196)               191100    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 447,691\n",
      "Trainable params: 447,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size : 128 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/vaibhav/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\", use_bias=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", dropout=0.2, recurrent_dropout=0.2)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 196)               191100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 447,691\n",
      "Trainable params: 447,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19826/19826 [==============================] - 4s 187us/step - loss: 0.5391 - acc: 0.8025\n",
      "Epoch 2/10\n",
      "19826/19826 [==============================] - 3s 134us/step - loss: 0.4014 - acc: 0.8524\n",
      "Epoch 3/10\n",
      "19826/19826 [==============================] - 3s 139us/step - loss: 0.3585 - acc: 0.8714\n",
      "Epoch 4/10\n",
      "19826/19826 [==============================] - 3s 145us/step - loss: 0.3191 - acc: 0.8860\n",
      "Epoch 5/10\n",
      "19826/19826 [==============================] - 3s 142us/step - loss: 0.2895 - acc: 0.8955\n",
      "Epoch 6/10\n",
      "19826/19826 [==============================] - 3s 143us/step - loss: 0.2604 - acc: 0.9043\n",
      "Epoch 7/10\n",
      "19826/19826 [==============================] - 3s 142us/step - loss: 0.2294 - acc: 0.9141\n",
      "Epoch 8/10\n",
      "19826/19826 [==============================] - 3s 140us/step - loss: 0.2066 - acc: 0.9236\n",
      "Epoch 9/10\n",
      "19826/19826 [==============================] - 3s 138us/step - loss: 0.1881 - acc: 0.9295\n",
      "Epoch 10/10\n",
      "19826/19826 [==============================] - 3s 137us/step - loss: 0.1627 - acc: 0.9367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f608f2390f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 128\n",
    "\n",
    "nb_filter = 128\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu', input_shape=(seq_length, 68)))\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu'))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# model.fit(X_train, y_train, batch_size=32, epochs=10)\n",
    "# score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_61 (Conv1D)           (None, 122, 128)          61056     \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 116, 128)          114816    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 38, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 36, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 30, 128)           114816    \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 28, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_16  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 389,635\n",
      "Trainable params: 389,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "f1 score:  0.8719042386109725\n",
      "avg recall 0.6973500814484704\n",
      "accuracy 0.8589872907000202\n"
     ]
    }
   ],
   "source": [
    "# model.summary()\n",
    "temp = model.predict(X_test)\n",
    "# evaluate(temp)\n",
    "preds = []\n",
    "\n",
    "for x in range(len(temp)):\n",
    "#     result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    result = temp[x]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)\n",
    "evaluate(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Embedding size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, recurrent_dropout=0.2, dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 30s - loss: 0.3706 - acc: 0.8655\n",
      "Epoch 2/14\n",
      " - 29s - loss: 0.2748 - acc: 0.9045\n",
      "Epoch 3/14\n",
      " - 29s - loss: 0.2597 - acc: 0.9096\n",
      "Epoch 4/14\n",
      " - 29s - loss: 0.2486 - acc: 0.9126\n",
      "Epoch 5/14\n",
      " - 29s - loss: 0.2418 - acc: 0.9160\n",
      "Epoch 6/14\n",
      " - 29s - loss: 0.2346 - acc: 0.9186\n",
      "Epoch 7/14\n",
      " - 33s - loss: 0.2277 - acc: 0.9217\n",
      "Epoch 8/14\n",
      " - 33s - loss: 0.2240 - acc: 0.9224\n",
      "Epoch 9/14\n",
      " - 32s - loss: 0.2153 - acc: 0.9256\n",
      "Epoch 10/14\n",
      " - 32s - loss: 0.2082 - acc: 0.9281\n",
      "Epoch 11/14\n",
      " - 29s - loss: 0.1999 - acc: 0.9311\n",
      "Epoch 12/14\n",
      " - 31s - loss: 0.1947 - acc: 0.9325\n",
      "Epoch 13/14\n",
      " - 39s - loss: 0.1899 - acc: 0.9347\n",
      "Epoch 14/14\n",
      " - 32s - loss: 0.1837 - acc: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faeed598f98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 8704 into shape (1,128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0ac3ad8bab56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 8704 into shape (1,128)"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9159048268356916\n",
      "avg recall 0.7493826692062546\n",
      "accuracy 0.9098244906193262\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 128 RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, recurrent_dropout=0.2, dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 27s - loss: 0.3935 - acc: 0.8588\n",
      "Epoch 2/14\n",
      " - 26s - loss: 0.2758 - acc: 0.9043\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.2581 - acc: 0.9084\n",
      "Epoch 4/14\n",
      " - 25s - loss: 0.2492 - acc: 0.9127\n",
      "Epoch 5/14\n",
      " - 26s - loss: 0.2463 - acc: 0.9142\n",
      "Epoch 6/14\n",
      " - 25s - loss: 0.2385 - acc: 0.9168\n",
      "Epoch 7/14\n",
      " - 25s - loss: 0.2308 - acc: 0.9183\n",
      "Epoch 8/14\n",
      " - 26s - loss: 0.2258 - acc: 0.9208\n",
      "Epoch 9/14\n",
      " - 26s - loss: 0.2210 - acc: 0.9219\n",
      "Epoch 10/14\n",
      " - 26s - loss: 0.2158 - acc: 0.9241\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.2091 - acc: 0.9251\n",
      "Epoch 12/14\n",
      " - 26s - loss: 0.2026 - acc: 0.9281\n",
      "Epoch 13/14\n",
      " - 26s - loss: 0.1988 - acc: 0.9316\n",
      "Epoch 14/14\n",
      " - 26s - loss: 0.1895 - acc: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29b05fe358>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9266472489443812\n",
      "avg recall 0.787312116442089\n",
      "accuracy 0.915473068388138\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, recurrent_dropout=0.2, dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               355152    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 867,743\n",
      "Trainable params: 867,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 27s - loss: 0.3690 - acc: 0.8684\n",
      "Epoch 2/14\n",
      " - 26s - loss: 0.2465 - acc: 0.9121\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.2199 - acc: 0.9198\n",
      "Epoch 4/14\n",
      " - 26s - loss: 0.1978 - acc: 0.9287\n",
      "Epoch 5/14\n",
      " - 26s - loss: 0.1735 - acc: 0.9375\n",
      "Epoch 6/14\n",
      " - 26s - loss: 0.1510 - acc: 0.9439\n",
      "Epoch 7/14\n",
      " - 26s - loss: 0.1290 - acc: 0.9520\n",
      "Epoch 8/14\n",
      " - 26s - loss: 0.1113 - acc: 0.9582\n",
      "Epoch 9/14\n",
      " - 26s - loss: 0.0915 - acc: 0.9665\n",
      "Epoch 10/14\n",
      " - 26s - loss: 0.0780 - acc: 0.9722\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.0684 - acc: 0.9751\n",
      "Epoch 12/14\n",
      " - 26s - loss: 0.0579 - acc: 0.9794\n",
      "Epoch 13/14\n",
      " - 25s - loss: 0.0518 - acc: 0.9811\n",
      "Epoch 14/14\n",
      " - 26s - loss: 0.0429 - acc: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f297854d710>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8968267868127393\n",
      "avg recall 0.7019873385287991\n",
      "accuracy 0.8948961065160379\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 26s - loss: 0.0372 - acc: 0.9872\n",
      "Epoch 2/14\n",
      " - 26s - loss: 0.0335 - acc: 0.9886\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.0287 - acc: 0.9908\n",
      "Epoch 4/14\n",
      " - 26s - loss: 0.0284 - acc: 0.9898\n",
      "Epoch 5/14\n",
      " - 26s - loss: 0.0270 - acc: 0.9903\n",
      "Epoch 6/14\n",
      " - 26s - loss: 0.0267 - acc: 0.9909\n",
      "Epoch 7/14\n",
      " - 26s - loss: 0.0199 - acc: 0.9926\n",
      "Epoch 8/14\n",
      " - 26s - loss: 0.0203 - acc: 0.9928\n",
      "Epoch 9/14\n",
      " - 26s - loss: 0.0178 - acc: 0.9938\n",
      "Epoch 10/14\n",
      " - 25s - loss: 0.0218 - acc: 0.9918\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.0190 - acc: 0.9932\n",
      "Epoch 12/14\n",
      " - 26s - loss: 0.0150 - acc: 0.9948\n",
      "Epoch 13/14\n",
      " - 26s - loss: 0.0142 - acc: 0.9952\n",
      "Epoch 14/14\n",
      " - 26s - loss: 0.0156 - acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f293432c400>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8798922681557714\n",
      "avg recall 0.6787232414703951\n",
      "accuracy 0.8823885414565261\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
