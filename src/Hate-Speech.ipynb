{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras import losses\n",
    "\n",
    "DATA_PATH=\"../data/\"\n",
    "DATA = DATA_PATH + \"labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter=',')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    data = [x[6] for x in raw]\n",
    "    return np.array(data)\n",
    "\n",
    "def getClass(raw):\n",
    "    classes = [x[5] for x in raw]\n",
    "    return np.array(classes)\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "    return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"F1 score:   \", f1)\n",
    "    print(\"Avg Recall: \", rec)    \n",
    "    print(\"Accuracy:   \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw = readData(DATA) \n",
    "tweets = getTweets(raw)\n",
    "classes = getClass(raw)\n",
    "tweets = preprocess(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[1:]\n",
    "classes = classes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x in tweets]\n",
    "X = np.delete(np.array(X), [0])\n",
    "y = np.delete(classes, [0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)\n",
    "vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6779519819556444\n",
      "Avg Recall:  0.3333333333333333\n",
      "Accuracy:    0.7758725035303611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'char',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(2, 6),\n",
    "    stop_words = en_stopwords)\n",
    "vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6779519819556444\n",
      "Avg Recall:  0.3333333333333333\n",
      "Accuracy:    0.7758725035303611\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1))\n",
    "vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6779519819556444\n",
      "Avg Recall:  0.3333333333333333\n",
      "Accuracy:    0.7758725035303611\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.6779519819556444\n",
      "Avg Recall:  0.3333333333333333\n",
      "Accuracy:    0.7758725035303611\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 68\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "maxlen = 128\n",
    "for doc in tweets:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = set(txt)\n",
    "vocab_size = len(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(data, char_indices):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [char_indices[w] for w in sentences]\n",
    "        x2 = np.eye(len(char_indices))[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vectorize_sentences(tweets, char_indices)\n",
    "y = to_categorical(classes)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_23 (Conv1D)           (None, 122, 128)          61056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 38, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 110,723\n",
      "Trainable params: 110,723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "seq_length = 128\n",
    "\n",
    "nb_filter = 128\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu', input_shape=(seq_length, 68)))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 5s - loss: 0.5800 - acc: 0.7875\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.4660 - acc: 0.8257\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.4043 - acc: 0.8526\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.3661 - acc: 0.8688\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.3354 - acc: 0.8816\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.3122 - acc: 0.8898\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.2934 - acc: 0.8974\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.2727 - acc: 0.9040\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.2556 - acc: 0.9116\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.2436 - acc: 0.9148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eec1f9d68>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, y_train, epochs = 10, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.894722387688959\n",
      "Avg Recall:  0.7025203893750982\n",
      "Accuracy:    0.8741174097236232\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict(X_test)\n",
    "preds = []\n",
    "\n",
    "for x in range(len(temp)):\n",
    "    result = temp[x]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)\n",
    "evaluate(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 122, 128)          61056     \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 116, 128)          114816    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 38, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 36, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 34, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 32, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 324,099\n",
      "Trainable params: 324,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "seq_length = 128\n",
    "\n",
    "nb_filter = 128\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu', input_shape=(seq_length, 68)))\n",
    "model.add(Conv1D(nb_filter, 7, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(Conv1D(nb_filter, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19826/19826 [==============================] - 7s 368us/step - loss: 0.5411 - acc: 0.7959\n",
      "Epoch 2/10\n",
      "19826/19826 [==============================] - 4s 192us/step - loss: 0.4043 - acc: 0.8487\n",
      "Epoch 3/10\n",
      "19826/19826 [==============================] - 4s 193us/step - loss: 0.3488 - acc: 0.8713\n",
      "Epoch 4/10\n",
      "19826/19826 [==============================] - 4s 200us/step - loss: 0.3164 - acc: 0.8853\n",
      "Epoch 5/10\n",
      "19826/19826 [==============================] - 4s 202us/step - loss: 0.2817 - acc: 0.8967\n",
      "Epoch 6/10\n",
      "19826/19826 [==============================] - 3s 167us/step - loss: 0.2468 - acc: 0.9078\n",
      "Epoch 7/10\n",
      "19826/19826 [==============================] - 4s 192us/step - loss: 0.2120 - acc: 0.9194\n",
      "Epoch 8/10\n",
      "19826/19826 [==============================] - 4s 198us/step - loss: 0.1861 - acc: 0.9274\n",
      "Epoch 9/10\n",
      "19826/19826 [==============================] - 4s 201us/step - loss: 0.1645 - acc: 0.9338\n",
      "Epoch 10/10\n",
      "19826/19826 [==============================] - 4s 202us/step - loss: 0.1499 - acc: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb1455b70>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, y_train, epochs = 10, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.8524946799492567\n",
      "Avg Recall:  0.6583308015538095\n",
      "Accuracy:    0.8523300383296348\n"
     ]
    }
   ],
   "source": [
    "temp = model.predict(X_test)\n",
    "preds = []\n",
    "\n",
    "for x in range(len(temp)):\n",
    "    result = temp[x]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)\n",
    "evaluate(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text To Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "allTweets = [x for x in tweets]\n",
    "classes = [x for x in classes]\n",
    "\n",
    "n = int(len(allTweets)*0.8)\n",
    "\n",
    "trainTweets = allTweets[:n]\n",
    "testTweets = allTweets[n:]\n",
    "trainClass = classes[:n]\n",
    "testClass = classes[n:]\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(trainTweets)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(trainTweets + testTweets)\n",
    "X_train = X[:n]\n",
    "X_test = X[n:]\n",
    "X_train = pad_sequences(X_train, maxlen = 32)\n",
    "X_test = pad_sequences(X_test, maxlen = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "for i in trainClass:\n",
    "    if i == \"0\":\n",
    "        Y_train.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_train.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_train.append([0, 0, 1])\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "for i in testClass:\n",
    "    if i == \"0\":\n",
    "        Y_test.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_test.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_test.append([0, 0, 1])\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, dropout=0.2, use_bias=True, recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", kernel_initializer=\"glorot_uniform\", recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 196)               191100    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 447,691\n",
      "Trainable params: 447,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 28s - loss: 0.3642 - acc: 0.8719\n",
      "Epoch 2/14\n",
      " - 27s - loss: 0.2542 - acc: 0.9118\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.2292 - acc: 0.9181\n",
      "Epoch 4/14\n",
      " - 27s - loss: 0.2085 - acc: 0.9256\n",
      "Epoch 5/14\n",
      " - 27s - loss: 0.1858 - acc: 0.9347\n",
      "Epoch 6/14\n",
      " - 27s - loss: 0.1656 - acc: 0.9399\n",
      "Epoch 7/14\n",
      " - 26s - loss: 0.1463 - acc: 0.9467\n",
      "Epoch 8/14\n",
      " - 27s - loss: 0.1279 - acc: 0.9533\n",
      "Epoch 9/14\n",
      " - 27s - loss: 0.1108 - acc: 0.9587\n",
      "Epoch 10/14\n",
      " - 27s - loss: 0.0996 - acc: 0.9642\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.0860 - acc: 0.9681\n",
      "Epoch 12/14\n",
      " - 27s - loss: 0.0767 - acc: 0.9722\n",
      "Epoch 13/14\n",
      " - 27s - loss: 0.0710 - acc: 0.9746\n",
      "Epoch 14/14\n",
      " - 27s - loss: 0.0601 - acc: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f31ae0390>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.8937814866545816\n",
      "Avg Recall:  0.6907232447257861\n",
      "Accuracy:    0.8922735525519467\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size : 128 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, dropout=0.2, use_bias=True, recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", kernel_initializer=\"glorot_uniform\", recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 196)               191100    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 447,691\n",
      "Trainable params: 447,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 28s - loss: 0.3711 - acc: 0.8664\n",
      "Epoch 2/14\n",
      " - 26s - loss: 0.2718 - acc: 0.9070\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.2584 - acc: 0.9097\n",
      "Epoch 4/14\n",
      " - 26s - loss: 0.2488 - acc: 0.9132\n",
      "Epoch 5/14\n",
      " - 26s - loss: 0.2400 - acc: 0.9153\n",
      "Epoch 6/14\n",
      " - 26s - loss: 0.2332 - acc: 0.9194\n",
      "Epoch 7/14\n",
      " - 26s - loss: 0.2260 - acc: 0.9220\n",
      "Epoch 8/14\n",
      " - 27s - loss: 0.2184 - acc: 0.9253\n",
      "Epoch 9/14\n",
      " - 26s - loss: 0.2101 - acc: 0.9280\n",
      "Epoch 10/14\n",
      " - 26s - loss: 0.2053 - acc: 0.9287\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.1959 - acc: 0.9323\n",
      "Epoch 12/14\n",
      " - 27s - loss: 0.1892 - acc: 0.9361\n",
      "Epoch 13/14\n",
      " - 26s - loss: 0.1851 - acc: 0.9359\n",
      "Epoch 14/14\n",
      " - 27s - loss: 0.1768 - acc: 0.9404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f28058128>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9134313144875803\n",
      "Avg Recall:  0.7290463026285119\n",
      "Accuracy:    0.9110349001412145\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, dropout=0.2, use_bias=True, recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", kernel_initializer=\"glorot_uniform\", recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 196)               266364    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 778,955\n",
      "Trainable params: 778,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 29s - loss: 0.3546 - acc: 0.8733\n",
      "Epoch 2/14\n",
      " - 27s - loss: 0.2494 - acc: 0.9118\n",
      "Epoch 3/14\n",
      " - 27s - loss: 0.2214 - acc: 0.9198\n",
      "Epoch 4/14\n",
      " - 27s - loss: 0.1949 - acc: 0.9298\n",
      "Epoch 5/14\n",
      " - 27s - loss: 0.1680 - acc: 0.9408\n",
      "Epoch 6/14\n",
      " - 27s - loss: 0.1429 - acc: 0.9488\n",
      "Epoch 7/14\n",
      " - 27s - loss: 0.1188 - acc: 0.9582\n",
      "Epoch 8/14\n",
      " - 27s - loss: 0.0987 - acc: 0.9636\n",
      "Epoch 9/14\n",
      " - 27s - loss: 0.0834 - acc: 0.9701\n",
      "Epoch 10/14\n",
      " - 26s - loss: 0.0701 - acc: 0.9744\n",
      "Epoch 11/14\n",
      " - 27s - loss: 0.0599 - acc: 0.9797\n",
      "Epoch 12/14\n",
      " - 27s - loss: 0.0526 - acc: 0.9811\n",
      "Epoch 13/14\n",
      " - 26s - loss: 0.0442 - acc: 0.9843\n",
      "Epoch 14/14\n",
      " - 27s - loss: 0.0401 - acc: 0.9854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb93acb38>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.8935686840112977\n",
      "Avg Recall:  0.694817403171914\n",
      "Accuracy:    0.8932822271535202\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Embedding Size : 256 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(196, dropout=0.2, use_bias=True, recurrent_initializer=\"orthogonal\", bias_initializer=\"zeros\", kernel_initializer=\"glorot_uniform\", recurrent_dropout=0.2, activation=\"tanh\", recurrent_activation=\"hard_sigmoid\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 196)               266364    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 778,955\n",
      "Trainable params: 778,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(GRU(lstm_out, activation='tanh', recurrent_activation='hard_sigmoid',  \n",
    "              use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 29s - loss: 0.3589 - acc: 0.8731\n",
      "Epoch 2/14\n",
      " - 26s - loss: 0.2693 - acc: 0.9075\n",
      "Epoch 3/14\n",
      " - 26s - loss: 0.2527 - acc: 0.9104\n",
      "Epoch 4/14\n",
      " - 27s - loss: 0.2429 - acc: 0.9155\n",
      "Epoch 5/14\n",
      " - 26s - loss: 0.2344 - acc: 0.9190\n",
      "Epoch 6/14\n",
      " - 26s - loss: 0.2235 - acc: 0.9237\n",
      "Epoch 7/14\n",
      " - 26s - loss: 0.2146 - acc: 0.9261\n",
      "Epoch 8/14\n",
      " - 26s - loss: 0.2037 - acc: 0.9303\n",
      "Epoch 9/14\n",
      " - 26s - loss: 0.1928 - acc: 0.9340\n",
      "Epoch 10/14\n",
      " - 26s - loss: 0.1856 - acc: 0.9358\n",
      "Epoch 11/14\n",
      " - 26s - loss: 0.1814 - acc: 0.9387\n",
      "Epoch 12/14\n",
      " - 26s - loss: 0.1697 - acc: 0.9434\n",
      "Epoch 13/14\n",
      " - 26s - loss: 0.1600 - acc: 0.9469\n",
      "Epoch 14/14\n",
      " - 26s - loss: 0.1518 - acc: 0.9491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eec1f9e80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9096601532904508\n",
      "Avg Recall:  0.7341576765232675\n",
      "Accuracy:    0.9043776477708292\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Embedding size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 35s - loss: 0.3925 - acc: 0.8600\n",
      "Epoch 2/14\n",
      " - 34s - loss: 0.2549 - acc: 0.9106\n",
      "Epoch 3/14\n",
      " - 33s - loss: 0.2255 - acc: 0.9191\n",
      "Epoch 4/14\n",
      " - 33s - loss: 0.2079 - acc: 0.9254\n",
      "Epoch 5/14\n",
      " - 33s - loss: 0.1893 - acc: 0.9313\n",
      "Epoch 6/14\n",
      " - 33s - loss: 0.1711 - acc: 0.9377\n",
      "Epoch 7/14\n",
      " - 34s - loss: 0.1533 - acc: 0.9428\n",
      "Epoch 8/14\n",
      " - 33s - loss: 0.1360 - acc: 0.9509\n",
      "Epoch 9/14\n",
      " - 34s - loss: 0.1208 - acc: 0.9562\n",
      "Epoch 10/14\n",
      " - 33s - loss: 0.1072 - acc: 0.9599\n",
      "Epoch 11/14\n",
      " - 33s - loss: 0.0937 - acc: 0.9660\n",
      "Epoch 12/14\n",
      " - 33s - loss: 0.0828 - acc: 0.9697\n",
      "Epoch 13/14\n",
      " - 33s - loss: 0.0751 - acc: 0.9736\n",
      "Epoch 14/14\n",
      " - 33s - loss: 0.0631 - acc: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb6dee9e8>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9035124321187848\n",
      "Avg Recall:  0.7094559678006201\n",
      "Accuracy:    0.9017550938067379\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi layer LSTM rmsprop Embedding size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)`\n",
      "  \n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32, 196)           254800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 196)               308112    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 819,503\n",
      "Trainable params: 819,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2, return_sequences = True))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 66s - loss: 0.3917 - acc: 0.8610\n",
      "Epoch 2/14\n",
      " - 63s - loss: 0.2795 - acc: 0.9054\n",
      "Epoch 3/14\n",
      " - 63s - loss: 0.2623 - acc: 0.9092\n",
      "Epoch 4/14\n",
      " - 63s - loss: 0.2561 - acc: 0.9126\n",
      "Epoch 5/14\n",
      " - 62s - loss: 0.2502 - acc: 0.9143\n",
      "Epoch 6/14\n",
      " - 63s - loss: 0.2446 - acc: 0.9164\n",
      "Epoch 7/14\n",
      " - 63s - loss: 0.2374 - acc: 0.9176\n",
      "Epoch 8/14\n",
      " - 63s - loss: 0.2322 - acc: 0.9173\n",
      "Epoch 9/14\n",
      " - 62s - loss: 0.2259 - acc: 0.9215\n",
      "Epoch 10/14\n",
      " - 63s - loss: 0.2212 - acc: 0.9229\n",
      "Epoch 11/14\n",
      " - 63s - loss: 0.2151 - acc: 0.9250\n",
      "Epoch 12/14\n",
      " - 63s - loss: 0.2127 - acc: 0.9261\n",
      "Epoch 13/14\n",
      " - 62s - loss: 0.2066 - acc: 0.9295\n",
      "Epoch 14/14\n",
      " - 63s - loss: 0.1979 - acc: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb8b60a90>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9267241247799751\n",
      "Avg Recall:  0.7566652675875654\n",
      "Accuracy:    0.918902562033488\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 128 RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 35s - loss: 0.3940 - acc: 0.8601\n",
      "Epoch 2/14\n",
      " - 33s - loss: 0.2767 - acc: 0.9037\n",
      "Epoch 3/14\n",
      " - 33s - loss: 0.2594 - acc: 0.9092\n",
      "Epoch 4/14\n",
      " - 33s - loss: 0.2509 - acc: 0.9126\n",
      "Epoch 5/14\n",
      " - 33s - loss: 0.2424 - acc: 0.9147\n",
      "Epoch 6/14\n",
      " - 33s - loss: 0.2374 - acc: 0.9162\n",
      "Epoch 7/14\n",
      " - 33s - loss: 0.2340 - acc: 0.9179\n",
      "Epoch 8/14\n",
      " - 33s - loss: 0.2269 - acc: 0.9194\n",
      "Epoch 9/14\n",
      " - 33s - loss: 0.2208 - acc: 0.9240\n",
      "Epoch 10/14\n",
      " - 33s - loss: 0.2156 - acc: 0.9249\n",
      "Epoch 11/14\n",
      " - 33s - loss: 0.2098 - acc: 0.9272\n",
      "Epoch 12/14\n",
      " - 32s - loss: 0.2041 - acc: 0.9284\n",
      "Epoch 13/14\n",
      " - 32s - loss: 0.1985 - acc: 0.9295\n",
      "Epoch 14/14\n",
      " - 33s - loss: 0.1918 - acc: 0.9319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb47d3dd8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9228497582049308\n",
      "Avg Recall:  0.756774319062377\n",
      "Accuracy:    0.9180956223522292\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 196)               355152    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 867,743\n",
      "Trainable params: 867,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 36s - loss: 0.3708 - acc: 0.8693\n",
      "Epoch 2/14\n",
      " - 32s - loss: 0.2463 - acc: 0.9135\n",
      "Epoch 3/14\n",
      " - 33s - loss: 0.2203 - acc: 0.9209\n",
      "Epoch 4/14\n",
      " - 32s - loss: 0.1997 - acc: 0.9270\n",
      "Epoch 5/14\n",
      " - 32s - loss: 0.1765 - acc: 0.9352\n",
      "Epoch 6/14\n",
      " - 33s - loss: 0.1524 - acc: 0.9448\n",
      "Epoch 7/14\n",
      " - 32s - loss: 0.1293 - acc: 0.9522\n",
      "Epoch 8/14\n",
      " - 32s - loss: 0.1145 - acc: 0.9579\n",
      "Epoch 9/14\n",
      " - 33s - loss: 0.0956 - acc: 0.9636\n",
      "Epoch 10/14\n",
      " - 32s - loss: 0.0798 - acc: 0.9708\n",
      "Epoch 11/14\n",
      " - 33s - loss: 0.0679 - acc: 0.9757\n",
      "Epoch 12/14\n",
      " - 33s - loss: 0.0563 - acc: 0.9787\n",
      "Epoch 13/14\n",
      " - 33s - loss: 0.0480 - acc: 0.9830\n",
      "Epoch 14/14\n",
      " - 33s - loss: 0.0425 - acc: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb2c0fc50>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9057149412855426\n",
      "Avg Recall:  0.7166753079708252\n",
      "Accuracy:    0.9027637684083115\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 256 RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/home/trip3r/.local/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 196)               355152    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 867,743\n",
      "Trainable params: 867,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 37s - loss: 0.3725 - acc: 0.8642\n",
      "Epoch 2/14\n",
      " - 33s - loss: 0.2718 - acc: 0.9060\n",
      "Epoch 3/14\n",
      " - 33s - loss: 0.2543 - acc: 0.9106\n",
      "Epoch 4/14\n",
      " - 33s - loss: 0.2465 - acc: 0.9136\n",
      "Epoch 5/14\n",
      " - 33s - loss: 0.2373 - acc: 0.9155\n",
      "Epoch 6/14\n",
      " - 34s - loss: 0.2303 - acc: 0.9181\n",
      "Epoch 7/14\n",
      " - 33s - loss: 0.2200 - acc: 0.9220\n",
      "Epoch 8/14\n",
      " - 33s - loss: 0.2139 - acc: 0.9258\n",
      "Epoch 9/14\n",
      " - 33s - loss: 0.2030 - acc: 0.9285\n",
      "Epoch 10/14\n",
      " - 31s - loss: 0.1932 - acc: 0.9318\n",
      "Epoch 11/14\n",
      " - 27s - loss: 0.1827 - acc: 0.9355\n",
      "Epoch 12/14\n",
      " - 27s - loss: 0.1755 - acc: 0.9383\n",
      "Epoch 13/14\n",
      " - 27s - loss: 0.1645 - acc: 0.9437\n",
      "Epoch 14/14\n",
      " - 27s - loss: 0.1524 - acc: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb29a5588>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.9054162418522184\n",
      "Avg Recall:  0.708635810671585\n",
      "Accuracy:    0.9035707080895703\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " - 27s - loss: 0.1412 - acc: 0.9497\n",
      "Epoch 2/14\n",
      " - 27s - loss: 0.1309 - acc: 0.9536\n",
      "Epoch 3/14\n",
      " - 27s - loss: 0.1217 - acc: 0.9577\n",
      "Epoch 4/14\n",
      " - 27s - loss: 0.1108 - acc: 0.9621\n",
      "Epoch 5/14\n",
      " - 27s - loss: 0.1005 - acc: 0.9663\n",
      "Epoch 6/14\n",
      " - 27s - loss: 0.0919 - acc: 0.9692\n",
      "Epoch 7/14\n",
      " - 27s - loss: 0.0829 - acc: 0.9710\n",
      "Epoch 8/14\n",
      " - 27s - loss: 0.0749 - acc: 0.9751\n",
      "Epoch 9/14\n",
      " - 27s - loss: 0.0661 - acc: 0.9768\n",
      "Epoch 10/14\n",
      " - 27s - loss: 0.0578 - acc: 0.9810\n",
      "Epoch 11/14\n",
      " - 27s - loss: 0.0525 - acc: 0.9828\n",
      "Epoch 12/14\n",
      " - 27s - loss: 0.0486 - acc: 0.9836\n",
      "Epoch 13/14\n",
      " - 27s - loss: 0.0464 - acc: 0.9840\n",
      "Epoch 14/14\n",
      " - 27s - loss: 0.0400 - acc: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eb29a5eb8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 14, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:    0.8987994048037962\n",
      "Avg Recall:  0.7110798365542689\n",
      "Accuracy:    0.8946943715957232\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
