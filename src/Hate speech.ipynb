{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras import losses\n",
    "\n",
    "DATA_PATH=\"../data/\"\n",
    "DATA = DATA_PATH + \"labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter=',')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    data = [x[6] for x in raw]\n",
    "    return np.array(data)\n",
    "\n",
    "def getClass(raw):\n",
    "    classes = [x[5] for x in raw]\n",
    "    return np.array(classes)\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "    return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"f1 score: \", f1)\n",
    "    print(\"avg recall\", rec)    \n",
    "    print(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw = readData(DATA) \n",
    "tweets = getTweets(raw)\n",
    "classes = getClass(raw)\n",
    "tweets = preprocess(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430 19190 4163\n"
     ]
    }
   ],
   "source": [
    "one, two, zero = 0, 0, 0\n",
    "for i in classes:\n",
    "    if i == '1':\n",
    "        one+=1\n",
    "    elif i == '2':\n",
    "        two+=1\n",
    "    elif i == '0':\n",
    "        zero+=1\n",
    "print(zero, one, two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8859701610631988\n",
      "avg recall 0.666017123630816\n",
      "accuracy 0.8987290700020173\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with Simple CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'char',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(2, 6),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8855780280279991\n",
      "avg recall 0.6655293753993855\n",
      "accuracy 0.898325600161388\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.886127565034215\n",
      "avg recall 0.6668150412533967\n",
      "accuracy 0.8987290700020173\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Tokens with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=0.1, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.8855780280279991\n",
      "avg recall 0.6655293753993855\n",
      "accuracy 0.898325600161388\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(train_features, y_train)\n",
    "y_predict = classifier.predict(test_features)\n",
    "evaluate(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x in tweets]\n",
    "X = np.delete(np.array(X), [0])\n",
    "y = np.delete(classes, [0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "vectorizer.fit(X)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "allTweets = [x for x in tweets]\n",
    "classes = [x for x in classes]\n",
    "\n",
    "n = int(len(allTweets)*0.8)\n",
    "\n",
    "trainTweets = allTweets[1:n]\n",
    "testTweets = allTweets[n:]\n",
    "trainClass = classes[1:n]\n",
    "testClass = classes[n:]\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(trainTweets)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(trainTweets + testTweets)\n",
    "X_train = X[:n-1]\n",
    "X_test = X[n-1:]\n",
    "X_train = pad_sequences(X_train, maxlen = 32)\n",
    "X_test = pad_sequences(X_test, maxlen = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 511,391\n",
      "Trainable params: 511,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding size : 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 32, 256)           512000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               355152    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 867,743\n",
      "Trainable params: 867,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X_train.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "for i in trainClass:\n",
    "    if i == \"0\":\n",
    "        Y_train.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_train.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_train.append([0, 0, 1])\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "for i in testClass:\n",
    "    if i == \"0\":\n",
    "        Y_test.append([1, 0, 0])\n",
    "    elif i == \"1\":\n",
    "        Y_test.append([0, 1, 0])\n",
    "    elif i == \"2\":\n",
    "        Y_test.append([0, 0, 1])\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 42s - loss: 0.3689 - acc: 0.8706\n",
      "Epoch 2/7\n",
      " - 46s - loss: 0.2474 - acc: 0.9132\n",
      "Epoch 3/7\n",
      " - 47s - loss: 0.2209 - acc: 0.9214\n",
      "Epoch 4/7\n",
      " - 44s - loss: 0.1985 - acc: 0.9276\n",
      "Epoch 5/7\n",
      " - 47s - loss: 0.1749 - acc: 0.9369\n",
      "Epoch 6/7\n",
      " - 47s - loss: 0.1519 - acc: 0.9429\n",
      "Epoch 7/7\n",
      " - 47s - loss: 0.1323 - acc: 0.9515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f68212eb8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, nb_epoch = 7, batch_size = batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == 0:\n",
    "        preds.append([1, 0, 0])\n",
    "    elif np.argmax(result) == 1:\n",
    "        preds.append([0, 1, 0])\n",
    "    elif np.argmax(result) == 2:\n",
    "        preds.append([0, 0, 1])\n",
    "        \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9076079746029507\n",
      "avg recall 0.7187095470339532\n",
      "accuracy 0.9063949969739762\n"
     ]
    }
   ],
   "source": [
    "evaluate(preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
